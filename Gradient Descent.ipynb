{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b95d55d",
   "metadata": {},
   "source": [
    "# What is Gradient Descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89929fe4",
   "metadata": {},
   "source": [
    "Gradient Descent is an iterative optimization algorithm used to minimize the cost function by adjusting the model's parameters (weights) step by step in the direction that reduces the cost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b8bb6",
   "metadata": {},
   "source": [
    "# Types of Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6af32",
   "metadata": {},
   "source": [
    "# 1. Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcd41e",
   "metadata": {},
   "source": [
    "Definition: Batch Gradient Descent calculates the gradient of the cost function for the entire dataset and updates the parameters once per iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f709764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_gradient_descent(X, y, lr=0.01, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(iterations):\n",
    "        gradient = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta -= lr * gradient\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37b5c9",
   "metadata": {},
   "source": [
    "# 2. Stochastic Gradient Descent (SGD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686310a5",
   "metadata": {},
   "source": [
    "Definition: Stochastic Gradient Descent updates the parameters for each training example, one at a time, instead of using the entire dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423bac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, lr=0.01, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(iterations):\n",
    "        for i in range(m):\n",
    "            gradient = X[i] * (X[i].dot(theta) - y[i])\n",
    "            theta -= lr * gradient\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3665e",
   "metadata": {},
   "source": [
    "# 3. Mini-Batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2925d90",
   "metadata": {},
   "source": [
    "Definition: Mini-Batch Gradient Descent splits the dataset into small batches and updates the parameters for each batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c029e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, y, lr=0.01, iterations=1000, batch_size=32):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(iterations):\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            gradient = (1/batch_size) * X_batch.T.dot(X_batch.dot(theta) - y_batch)\n",
    "            theta -= lr * gradient\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b18e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
